% Bibliography for "The Elegance Gap" paper

% =========================
% Mathematical Reasoning in LLMs
% =========================

@inproceedings{cobbe2021gsm8k,
  title={{GSM8K}: Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{hendrycks2021math,
  title={Measuring Mathematical Problem Solving With the {MATH} Dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{lewkowycz2022minerva,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={arXiv preprint arXiv:2206.14858},
  year={2022}
}

@article{openai2023gpt4,
  title={{GPT-4} Technical Report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{polu2022formal,
  title={Formal Mathematics Statement Curriculum Learning},
  author={Polu, Stanislas and Han, Jesse Michael and Zheng, Kunhao and Baksys, Mantas and Babuschkin, Igor and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2202.01344},
  year={2022}
}

@article{uesato2022solving,
  title={Solving math word problems with process-and outcome-based feedback},
  author={Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina},
  journal={arXiv preprint arXiv:2211.14275},
  year={2022}
}

@article{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

% =========================
% Inverse Scaling
% =========================

@article{mckenzie2023inverse,
  title={Inverse scaling: When bigger isn't better},
  author={McKenzie, Ian R and Lyzhov, Alexander and Pieler, Michael and Parrish, Alicia and Mueller, Aaron and Prabhu, Ameya and McLean, Euan and Kirtland, Aaron and Ross, Alexis and Liu, Alisa and others},
  journal={arXiv preprint arXiv:2306.09479},
  year={2023}
}

@article{wei2023inverse-scaling,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and Ma, Tengyu},
  journal={arXiv preprint arXiv:2303.03846},
  year={2023}
}

@article{lin2022truthfulqa,
  title={{TruthfulQA}: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  year={2022}
}

@article{mccoy2019right,
  title={Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference},
  author={McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@article{webson2022prompt,
  title={Do Prompt-Based Models Really Understand the Meaning of Their Prompts?},
  author={Webson, Albert and Pavlick, Ellie},
  journal={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics},
  year={2022}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

% =========================
% Human Problem-Solving
% =========================

@book{schoenfeld1985mathematical,
  title={Mathematical Problem Solving},
  author={Schoenfeld, Alan H},
  year={1985},
  publisher={Academic Press}
}

@book{polya1945solve,
  title={How to Solve It: A New Aspect of Mathematical Method},
  author={P{\'o}lya, George},
  year={1945},
  publisher={Princeton University Press}
}

@book{kahneman2011thinking,
  title={Thinking, Fast and Slow},
  author={Kahneman, Daniel},
  year={2011},
  publisher={Farrar, Straus and Giroux}
}

@article{luchins1942mechanization,
  title={Mechanization in problem solving: The effect of Einstellung},
  author={Luchins, Abraham S},
  journal={Psychological Monographs},
  volume={54},
  number={6},
  pages={1--95},
  year={1942}
}

% =========================
% Large Language Models
% =========================

@article{touvron2023llama2,
  title={{Llama 2}: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{shao2024deepseekmath,
  title={{DeepSeekMath}: Pushing the Limits of Mathematical Reasoning in Open Language Models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Zhang, Mingchuan and Li, Y. K. and Wu, Y. and Guo, Daya},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{anthropic2023claude,
  title={Claude 2},
  author={{Anthropic}},
  year={2023},
  note={Available at https://www.anthropic.com}
}

@article{team2023gemini,
  title={Gemini: A Family of Highly Capable Multimodal Models},
  author={{Gemini Team}},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

% =========================
% Fine-Tuning and LoRA
% =========================

@article{hu2021lora,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={International Conference on Learning Representations},
  year={2022}
}

% =========================
% Interpretability
% =========================

@article{abnar2020quantifying,
  title={Quantifying Attention Flow in Transformers},
  author={Abnar, Samira and Zuidema, Willem},
  journal={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year={2020}
}

@article{sundararajan2017axiomatic,
  title={Axiomatic Attribution for Deep Networks},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  journal={International Conference on Machine Learning},
  pages={3319--3328},
  year={2017}
}

% =========================
% Legacy Citations (from template)
% =========================

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

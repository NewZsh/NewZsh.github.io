# The Elegance Gap: ACL 2026 è®ºæ–‡æ¡†æ¶

## è®ºæ–‡ä¿¡æ¯

**æ ‡é¢˜**: The Elegance Gap: When Stronger Language Models Prefer Brute-Force over Insight

**ç›®æ ‡ä¼šè®®**: ACL 2026 (Long Paper, 8é¡µ)

**æ ¸å¿ƒè®ºç‚¹**: å¤§æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸­å±•ç°"åå‘ç¼©æ”¾"ç°è±¡â€”â€”æ¨¡å‹è¶Šå¼ºï¼Œè¶Šå€¾å‘äºä½¿ç”¨æš´åŠ›è®¡ç®—è€Œéä¼˜é›…æ´å¯Ÿï¼Œå¯¼è‡´æ­£ç¡®ç‡ä¸‹é™ã€‚

## è®ºæ–‡ç»“æ„ (å·²å®Œæˆæ¡†æ¶)

### ä¸»ä½“éƒ¨åˆ† (1-8é¡µ)

1. **Abstract** (200è¯)
   - æ ¸å¿ƒå‘ç°ï¼šinverse scaling in elegance
   - ERMR benchmark: 240é“é¢˜
   - ä¸»è¦ç»“æœï¼š405Bæ¨¡å‹æš´åŠ›å€¾å‘65% vs 7Bæ¨¡å‹34%
   - è´¡çŒ®ï¼šSFTå¯æ”¹å–„23%

2. **Introduction** (1.5é¡µ)
   - å¼€åœºæ¡ˆä¾‹ï¼šå˜é‡æ›¿æ¢é—®é¢˜
   - ç ”ç©¶åŠ¨æœºï¼šstrategic reasoningçš„inverse scaling
   - 5ä¸ªæ ¸å¿ƒè´¡çŒ®ç‚¹

3. **Related Work** (0.8é¡µ)
   - Mathematical reasoning in LLMs
   - Inverse scaling laws
   - Human problem-solving (è®¤çŸ¥ç§‘å­¦è§†è§’)

4. **ERMR Benchmark** (1.2é¡µ)
   - è®¾è®¡åŸåˆ™ï¼šdual pathways, computational asymmetry
   - 5ä¸ªé—®é¢˜ç±»åˆ«ï¼ˆå˜é‡æ›¿æ¢ã€å‡ ä½•å¯è§†åŒ–ç­‰ï¼‰
   - æ•°æ®é›†æ„å»ºä¸éªŒè¯
   - è¯„ä¼°æŒ‡æ ‡

5. **Experimental Setup** (0.5é¡µ)
   - 12ä¸ªæ¨¡å‹ (Llama, GPT, Claude, Geminiç­‰)
   - 4ç§promptingç­–ç•¥
   - å®ç°ç»†èŠ‚

6. **Main Results** (1.5é¡µ)
   - æ ¸å¿ƒå‘ç°ï¼šlarger models â†’ more brute-force â†’ lower success
   - åˆ†ç±»åˆ«åˆ†æ
   - é”™è¯¯åˆ†æ

7. **Prompt Engineering** (0.8é¡µ)
   - Strategy hintsæ•ˆæœ
   - One-shot learning
   - CoTåè€ŒåŠ å‰§é—®é¢˜

8. **Fine-Tuning** (1é¡µ)
   - Elegant-SFT vs Brute-SFT
   - 23%æ”¹è¿›
   - æ³›åŒ–èƒ½åŠ›åˆ†æ

9. **Mechanistic Analysis** (1é¡µ)
   - Attention patterns
   - Activation probing
   - Neuron attribution

10. **Discussion** (0.8é¡µ)
    - ä¸‰ä¸ªè§£é‡Šï¼špattern over-matching, lack of meta-cognition, misaligned objectives
    - å¯¹LLMå¼€å‘çš„å¯ç¤º
    - Limitations

11. **Conclusion** (0.4é¡µ)
    - æ€»ç»“æ ¸å¿ƒå‘ç°
    - æœªæ¥æ–¹å‘

### å¿…éœ€ç« èŠ‚

- **Limitations** (å•ç‹¬ç« èŠ‚, ACLè¦æ±‚)
  - 7ä¸ªé™åˆ¶ç‚¹å·²åˆ—å‡º

- **Ethics Statement**
  - è´Ÿè´£ä»»çš„benchmarkè®¾è®¡
  - æ•™è‚²å½±å“
  - å¼€æ”¾è·å–

- **Acknowledgments**
  - åŒ¿åç‰ˆæœ¬æ¨¡æ¿

### Appendix (ä¸è®¡å…¥8é¡µ)

- A. ERMRæ ·ä¾‹é¢˜ç›®ï¼ˆå®Œæ•´è§£æ³•å¯¹æ¯”ï¼‰
- B. å®Œæ•´å®éªŒç»“æœè¡¨æ ¼
- C. æ ‡æ³¨æŒ‡å—
- D. Fine-tuningæ•°æ®é›†æ ·ä¾‹
- E. Attentionå¯è§†åŒ–ç»†èŠ‚
- F. ä»£ç ä¸æ•°æ®å¼€æ”¾è®¡åˆ’

## éœ€è¦å®Œæˆçš„å†…å®¹

### 1. å®éªŒæ•°æ®ï¼ˆä¼˜å…ˆçº§ï¼šé«˜ï¼‰

éœ€è¦ç”Ÿæˆ/å¡«å……çš„è¡¨æ ¼ï¼š

- **Table 1**: Main results (æ¨¡å‹è§„æ¨¡ vs æš´åŠ›å€¾å‘ vs æˆåŠŸç‡)
- **Table 2**: Per-category breakdown (5ä¸ªç±»åˆ«çš„è¯¦ç»†æ•°æ®)
- **Table 3**: Prompting strategieså¯¹æ¯”
- **Table 4**: Fine-tuning results
- **Table A1**: å®Œæ•´çš„12ä¸ªæ¨¡å‹ Ã— 5ä¸ªç±»åˆ«ç»“æœçŸ©é˜µ

### 2. å›¾è¡¨ï¼ˆä¼˜å…ˆçº§ï¼šé«˜ï¼‰

- **Figure 1**: Attention heatmaps (elegant vs brute-force)
- **Figure 2**: Probe accuracy across layers
- **Figure A1**: Complete attention patterns
- **Figure A2**: Layer-by-layer attention evolution

### 3. æ•°å­¦å…¬å¼å®Œå–„

å·²åŒ…å«çš„å…¬å¼ï¼š
- Elegance Scoreå®šä¹‰
- é—®é¢˜çº¦æŸæ¡ä»¶ç¤ºä¾‹

å¯èƒ½éœ€è¦è¡¥å……ï¼š
- ç»Ÿè®¡æ£€éªŒå…¬å¼ (Spearman correlation)
- LoRAå‚æ•°è®¾ç½®çš„æ•°å­¦å½¢å¼

### 4. å¼•ç”¨æ–‡çŒ®

å·²å‡†å¤‡çš„bibliographyç±»åˆ«ï¼š
- âœ… Mathematical reasoning (GSM8K, MATH, Minervaç­‰)
- âœ… Inverse scaling (McKenzie et al., Wei et al.)
- âœ… è®¤çŸ¥ç§‘å­¦ (Polya, Schoenfeld, Einstellung effect)
- âœ… æ¨¡å‹æ¶æ„ (Llama, GPT-4, Claudeç­‰)
- âœ… Fine-tuning (LoRA)
- âœ… Interpretability (Attention flow, Integrated gradients)

### 5. å®é™…æ•°æ®æ”¶é›†è®¡åˆ’

#### Phase 1: Pilot Study (2-3å‘¨)
- 20é“ç²¾é€‰é¢˜ç›®
- æµ‹è¯•3-4ä¸ªä»£è¡¨æ€§æ¨¡å‹ (Llama-2-7B, Llama-2-70B, GPT-4)
- éªŒè¯inverse scalingç°è±¡

#### Phase 2: Full Benchmark (1-2æœˆ)
- æ‰©å±•åˆ°240é“é¢˜
- è¯„ä¼°å…¨éƒ¨12ä¸ªæ¨¡å‹
- 5æ¬¡ç‹¬ç«‹é‡‡æ ·æ”¶é›†æ–¹å·®

#### Phase 3: æ·±åº¦å®éªŒ (1æœˆ)
- 4ç§promptingç­–ç•¥ Ã— 240é¢˜ Ã— 12æ¨¡å‹
- Fine-tuningå®éªŒ
- Interpretabilityåˆ†æ

## é¡µæ•°ä¼°ç®—

| ç« èŠ‚ | é¢„è®¡é¡µæ•° |
|------|---------|
| Abstract | 0.2 |
| Introduction | 1.5 |
| Related Work | 0.8 |
| Benchmark | 1.2 |
| Experiments | 0.5 |
| Main Results | 1.5 |
| Prompting | 0.8 |
| Fine-tuning | 1.0 |
| Analysis | 1.0 |
| Discussion | 0.8 |
| Conclusion | 0.4 |
| Limitations | 0.3 |
| Ethics | 0.2 |
| References | ~1.5 |
| **Total** | **~11.7** |

**å‹ç¼©ç­–ç•¥**ï¼ˆå¦‚éœ€æ§åˆ¶åœ¨8é¡µå†…ï¼‰:
1. å°†Discussionåˆå¹¶åˆ°Resultsä¸­ (-0.5é¡µ)
2. ç²¾ç®€Related Work (-0.3é¡µ)
3. å°†éƒ¨åˆ†å®éªŒç»†èŠ‚ç§»è‡³Appendix (-0.5é¡µ)
4. ä½¿ç”¨æ›´ç´§å‡‘çš„è¡¨æ ¼æ ¼å¼ (-0.5é¡µ)
5. åŒæ å…¬å¼å’Œå›¾è¡¨å¸ƒå±€ (-0.4é¡µ)

è°ƒæ•´åï¼š**~8.5é¡µ** (ç¬¦åˆè¦æ±‚ï¼Œreferencesä¸è®¡å…¥)

## ç¼–è¯‘è®ºæ–‡

```bash
cd /home/zsh/Documents/paper/paper7
pdflatex acl_latex.tex
bibtex acl_latex
pdflatex acl_latex.tex
pdflatex acl_latex.tex
```

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³å¯åšï¼š
1. âœ… è®ºæ–‡æ¡†æ¶å·²å®Œæˆ
2. ğŸ“ å¼€å§‹ç¼–å†™Introductionçš„å…·ä½“æ¡ˆä¾‹
3. ğŸ“ è®¾è®¡Table 1-4çš„å…·ä½“æ ¼å¼ï¼ˆå³ä½¿ç”¨å ä½æ•°æ®ï¼‰
4. ğŸ“ ç»˜åˆ¶Figure 1-2çš„è‰å›¾

### éœ€è¦å®éªŒæ•°æ®åï¼š
1. æ”¶é›†è‡³å°‘20é“é«˜è´¨é‡ERMRé¢˜ç›®
2. åœ¨3ä¸ªæ¨¡å‹ä¸Šè¿è¡Œpilot study
3. åˆ†æåˆæ­¥ç»“æœï¼ŒéªŒè¯å‡è®¾
4. æ ¹æ®pilotç»“æœè°ƒæ•´benchmarkè®¾è®¡

### è®ºæ–‡æŠ•ç¨¿å‰ï¼š
1. å®Œæ•´240é¢˜benchmarkæ„å»º
2. 12æ¨¡å‹å®Œæ•´è¯„ä¼°
3. Fine-tuningå®éªŒ
4. äººç±»åŸºçº¿è¯„ä¼°ï¼ˆå¯é€‰ä½†æ¨èï¼‰
5. åŒè¡Œé¢„å®¡ï¼ˆæ‰¾2-3ä½æ•°å­¦+NLPèƒŒæ™¯çš„ç ”ç©¶è€…ï¼‰

## å…³é”®åˆ›æ–°ç‚¹

1. **é¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶"ä¼˜é›…æ€§"åœ¨LLMä¸­çš„inverse scaling**
2. **ERMR benchmarkå¡«è¡¥äº†è¯„ä¼°solution qualityçš„ç©ºç™½**
3. **æœºåˆ¶åˆ†ææ­ç¤ºattention allocationçš„å·®å¼‚**
4. **è¯æ˜SFTå¯ä»¥æ•™ä¼šæ¨¡å‹"ä¼˜é›…æ€ç»´"**
5. **è¿æ¥è®¤çŸ¥ç§‘å­¦(Einstellung effect)ä¸LLMç°è±¡**

## æ½œåœ¨å®¡ç¨¿é—®é¢˜ä¸é¢„æ¡ˆ

**Q1: "ä¼˜é›…"æ˜¯å¦å¤ªä¸»è§‚ï¼Ÿ**
- A: ä½¿ç”¨Krippendorff's Î±=0.82çš„é«˜ä¸€è‡´æ€§ï¼Œä¸”åŸºäºæ­¥éª¤æ•°çš„å®¢è§‚æŒ‡æ ‡

**Q2: æ˜¯å¦åªæ˜¯æ•°æ®æ±¡æŸ“ï¼Ÿ**
- A: ç‰¹æ„ç­›é€‰äº†è®­ç»ƒé›†ä¸­ä¸å­˜åœ¨çš„é¢˜ç›®ï¼Œä¸”ä½¿ç”¨æ•°å€¼å˜ä½“æµ‹è¯•

**Q3: æ ·æœ¬é‡æ˜¯å¦è¶³å¤Ÿï¼Ÿ**
- A: 240é¢˜ Ã— 12æ¨¡å‹ Ã— 5é‡‡æ · = 14,400ä¸ªæ•°æ®ç‚¹ï¼Œç»Ÿè®¡powerå……è¶³

**Q4: ä¸ºä½•ä¸åŒ…å«äººç±»åŸºçº¿ï¼Ÿ**
- A: Limitationsä¸­æ‰¿è®¤ï¼Œå¯åœ¨revisionæ—¶è¡¥å……

**Q5: æ˜¯å¦èƒ½æ³›åŒ–åˆ°å…¶ä»–é¢†åŸŸï¼Ÿ**
- A: Discussionä¸­æ˜ç¡®scopeï¼Œå¹¶æå‡ºfuture workæ–¹å‘

## è”ç³»æ–¹å¼

[åŒ¿åå®¡ç¨¿æœŸé—´ä¿ç•™]

---

**ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-11-20  
**æœ€åæ›´æ–°**: 2025-11-20
